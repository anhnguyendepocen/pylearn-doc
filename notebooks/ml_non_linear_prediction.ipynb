{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "SVM are based kernel methods require only a user-specified kernel function $K(x_i, x_j)$, i.e., a **similarity function** over pairs of data points $(x_i, x_j)$ into kernel (dual) space on which learning algorithms operate linearly, i.e. every operation on points is a linear combination of $K(x_i, x_j)$.\n",
    "\n",
    "Outline of the SVM algorithm:\n",
    "\n",
    "1. Map points $x$ into kernel space using a kernel function: $x \\rightarrow K(x, .)$.\n",
    "\n",
    "2. Learning algorithms operate linearly by dot product into high-kernel space $K(., x_i) \\cdot K(., x_j)$.\n",
    "    - Using the kernel trick (Mercer’s Theorem) replace dot product in hgh dimensional space by a simpler operation such that $K(., x_i) \\cdot K(., x_j) = K(x_i, x_j)$. Thus we only need to compute a similarity measure  for each pairs of point and store in a $N \\times N$ Gram matrix.\n",
    "    - Finally, The learning process consist of estimating the $\\alpha_i$ of the decision function that maximises the hinge loss (of $f(x)$) plus some penalty when applied on all training points.\n",
    "$$\n",
    "f(x) = \\text{sign} \\left(\\sum_i^N \\alpha_i~y_i~K(x_i, x)\\right). \n",
    "$$\n",
    "\n",
    "3. Predict a new point $x$ using the decision function.\n",
    "\n",
    "![Support Vector Machines.](images/svm_rbf_kernel_mapping_and_decision_function.png)\n",
    "\n",
    "\n",
    "\n",
    "### Gaussian kernel (RBF, Radial Basis Function):\n",
    "\n",
    "One of the most commonly used kernel is the Radial Basis Function (RBF) Kernel. For a pair of points $x_i, x_j$ the RBF kernel is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "    K(x_i, x_j) &= \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\\\\\n",
    "    &= \\exp\\left(-\\gamma~\\|x_i - x_j\\|^2\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Where $\\sigma$ (or $\\gamma$)  defines the kernel width parameter. Basically, we consider a Gaussian function centered on each training sample $x_i$.  it has a ready interpretation as a similarity measure as it deacreses with squared Euclidean distance between the two feature vectors.\n",
    "\n",
    "Non linear SVM also exists for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Errors: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset\n",
    "X, y = datasets.make_classification(n_samples=10, n_features=2,n_redundant=0,\n",
    "                                    n_classes=2,\n",
    "                                    random_state=1,\n",
    "                                    shuffle=False)\n",
    "clf = SVC(kernel='rbf')#, gamma=1)\n",
    "clf.fit(X, y)\n",
    "print(\"#Errors: %i\" % np.sum(y != clf.predict(X)))\n",
    "\n",
    "clf.decision_function(X)\n",
    "\n",
    "# Usefull internals:\n",
    "# Array of support vectors\n",
    "clf.support_vectors_\n",
    "\n",
    "# indices of support vectors within original X\n",
    "np.all(X[clf.support_,:] == clf.support_vectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "A random forest is a meta estimator that fits a number of **decision tree learners** on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "### Decision tree learner\n",
    "\n",
    "A tree can be \"learned\" by splitting the trainind dataset into subsets based on an features value test.\n",
    "\n",
    "Each internal node represents a \"test\" on an feature resulting on the split of the current sample. At each step the algorithm setects the feature and a cutoff value that miximises a given metric. Different metrics exist for regression tree (target is continuous) or classification tree (the target is qualitative).\n",
    "\n",
    "This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This general principle is implemented by many recursive partitioning tree algorithms.\n",
    "\n",
    "![Classification tree.](images/classification_tree.jpg)\n",
    "\n",
    "Decision trees are simple to understand and interpret however they tend to overfit the data. However decision trees tend to overfitt the training set.  Leo Breiman propose random forest to deal with this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest.fit(X, y)\n",
    "\n",
    "print(\"#Errors: %i\" % np.sum(y != forest.predict(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left out samples validation\n",
    "\n",
    "The **training error** can be easily calculated by applying the statistical learning method to the observations used in its training. But because of overfitting, the training error rate can dramatically underestimate the error that would be obtained on new samples.\n",
    "\n",
    "\n",
    "The **test error** is the average error that results from a learning method to predict the response on a new samples that is, on samples that were not used in training the method. Given a data set, the use of a particular learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case.\n",
    "\n",
    "Thus the original dataset is generally splited in a training and a test (or validation) data sets. Large training set (80%) small test set (20%) might provide a poor estimation of the predictive performances. On the contrary, large test set and small training set might produce a poorly estimated learner. This is why, on situation where we cannot afford such split, it recommended to use cross-Validation scheme to estimate the predictive power of a learning algorithm.\n",
    "\n",
    "\n",
    "## Cross-Validation (CV)\n",
    "\n",
    "Cross-Validation scheme randomly divid the set of observations into $K$ groups, or **folds**, of approximately equal size. The first fold is treated as a validation set, and the method $f()$ is fitted on the remaining union of $K - 1$ folds: ($f(X_{-K})$).\n",
    "\n",
    "The mean error measure (generally a loss function) is evaluated of the on the observations in the held-out fold. For each sample $i$ we consider the model estimated on the data set that did not contain it, noted $-K(i)$. This procedure is repeated $K$ times; each time, a different group of observations is treated as a test set.\n",
    "Then we compare the predicted value ($f(X_{-K(i)}) = \\hat{y_i})$ with true value $y_i$ using a Error function $L()$. Then the cross validation estimate of prediction error is\n",
    "\n",
    "$$\n",
    "CV(f) = \\frac{1}{N} \\sum_i^N L\\left(y_i, f(X_{-K(i)}) \\right).\n",
    "$$\n",
    "\n",
    "This validation scheme is known as the **K-Fold CV**. The extreme case where $K = N$ is known as **leave-one-out cross-validation, LOO CV**.\n",
    "\n",
    "Model assessment: having chosen a final model, estimating its predic-\n",
    "tion error (generalization error) on new data.\n",
    "\n",
    "Model selection: estimating the performance of different models in order\n",
    "to choose the best one.\n",
    "\n",
    "to randomly divide the dataset into three parts: a training set, a validation\n",
    "set, and a test set. The training set is used to fit the models; the validation\n",
    "set is used to estimate prediction error for model selection; the test set is\n",
    "used for assessment of the generalization error of the final chosen model.\n",
    "\n",
    "**Sources**\n",
    "* ISLR\n",
    "* Andreas Mueller https://github.com/amueller/pydata-strata-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV for model selection: setting the hyper parameters\n",
    "**Sources**\n",
    "* ISLR\n",
    "* Andreas Mueller https://github.com/amueller/pydata-strata-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstraping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

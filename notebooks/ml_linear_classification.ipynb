{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's linear discriminant with equal class covariance\n",
    "\n",
    "This geometric method does not make probabilistic assumption, it only relies on distances. It look for the **linear projection** (rotation) $\\mathbf{w}$ that maximizes the between / within variance ratio: noted $F(w)$. It should be considered as a pedagogical experimental method. However with few assumptions it will provide the same results than Linear discriminant analysis (LDA) explained below.\n",
    "\n",
    "Suppose two classes ($C_0, C_1$) of observations have means $\\mu_0$, $\\mu_1$ and the same the \n",
    "total within-class scatter ``covariance'' matrix $S_W$ given by:\n",
    "\\begin{align}\n",
    "S_W &= \\sum_{i\\in C_0} (x_i - \\mu_0)(x_i - \\mu_0)^T + \\sum_{j\\in C_1} (x_j - \\mu_1)(x_j -\\mu_1)^T\\\\\n",
    "    &= X_c^T X_c\n",
    "\\end{align}\n",
    "\n",
    "Where $X_c$ is the $(N \\times P)$ matrix of data centered on their respective means:\n",
    "\n",
    "$$\n",
    "X_c = \n",
    "\\begin{bmatrix}\n",
    "X_0 -  \\mu_0 \\\\ X_1 -  \\mu_1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $X_0$ and $X_1$ are the $(N_0 \\times P)$ and $(N_1 \\times P)$ matrices of samples of classes $C_0$ and $C_1$.\n",
    "\n",
    "Let $S_B$ being the scatter ``between-class'' covariance matrix and given by\n",
    "\n",
    "$$\n",
    "S_B = (\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T\n",
    "$$\n",
    "\n",
    "\n",
    "The linear combination of features $w^T x$ have means $w^T \\mu_i$ for i=0,1 and variance $w^T \n",
    "X^T_c X_c w$. Fisher defined the separation between these two distributions to be the ratio of the \n",
    "variance between the classes to the variance within the classes:\n",
    "\n",
    "\\begin{align}\n",
    "F_{\\text{Fisher}}(w) &= \\frac{\\sigma_{\\text{between}}^2}{\\sigma_{\\text{within}}^2}\\\\\n",
    "                     &= \\frac{(w^T \\mu_1 - w^T \\mu_0)^2}{w^T  X^T_c X_c w}\\\\\n",
    "                     &= \\frac{(w^T (\\mu_1 - \\mu_0))^2}{w^T  X^T_c X_c w}\\\\ \n",
    "                     &= \\frac{w^T (\\mu_1 - \\mu_0) (\\mu_1 - \\mu_0)^T w}{w^T X^T_c X_c w}\\\\\n",
    "                     &= \\frac{w^T S_B w}{w^T S_W w}\n",
    "\\end{align}\n",
    "\n",
    "### The Fisher most discriminant projection\n",
    "\n",
    "In the two classes case, the maximum separation occurs by a projection on the $(\\mu_1 - \\mu_0)$ using the Mahalanobis \n",
    "metric $S_B^{-1}$:\n",
    "\n",
    "$$\n",
    "    w \\propto S_B^{-1}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "Differentiating $F_{Fisher}(w)$ with respect to $w$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{w}F_{Fisher}(w) &= 0\\\\\n",
    "\\nabla_{w}(\\frac{w^T S_B w}{w^T S_W w}) &= 0\\\\\n",
    "(w^T S_W w)(2 S_B w) - (w^T S_B w)(2 S_W w) &= 0\\\\\n",
    "(w^T S_W w)(S_B w) &= (w^T S_B w)(S_W w)\\\\\n",
    "S_B w &= \\frac{w^T S_B w}{w^T S_W w}(S_W w)\\\\\n",
    "S_B w &= \\lambda (S_W w)\\\\\n",
    "S_W^{-1}{S_B} w &= \\lambda  w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since we do not care about the magnitude of $w$, only its direction, we replaced the scalar factor $(w^T S_B w) / (w^T S_W w)$ by $\\lambda$. \n",
    "\n",
    "In the multiple $K$-classes case, the solutions $w$ are determined by the eigenvectors of $S_W^{-1}{S_B}$ that correspond to the ($K-1$) largest eigenvalues.\n",
    "\n",
    "However, in the two classes case (where $S_B = (\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T$) it is easy to \n",
    "show that $w = S_W^{-1}(\\mu_1 - \\mu_0)$ is the unique eigenvector of $S_W^{-1}{S_B}$:\n",
    "\n",
    "\\begin{align*}\n",
    "S_W^{-1}(\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T w &= \\lambda  w\\\\\n",
    "S_W^{-1}(\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T S_W^{-1}(\\mu_1 - \\mu_0) &= \\lambda  S_W^{-1}(\\mu_1 \n",
    "- \\mu_0)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where here $\\lambda = (\\mu_1 - \\mu_0 )^T S_W^{-1}(\\mu_1 - \\mu_0)$. Which leads to the result:\n",
    "$$\n",
    "    w \\propto S_B^{-1}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "### The separating hyperplane\n",
    "\n",
    "The separating hyperplane is a plan $P-1$-dimensional hyper surface orthogonal to the projection vector: $w$. To define the origin of the plane along $w$. This origin is the classification threshold to decide whether a point should be classified $C_0$ or $C_1$. The threshold can be chosen as the hyperplane between projections of the two means:\n",
    "$$\n",
    "T = w \\cdot \\frac{1}{2}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "![The Fisher most discriminant projection](images/fisher_linear_disc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis (LDA)\n",
    "\n",
    "Linear discriminant analysis (LDA) is a probabilistic generalization of Fisher's linear discriminant. It uses Bayes' rule to fix the threshold based on prior probabilities of classes. \n",
    "\n",
    "1. First compute the The class-**conditional distributions** of $x$ given class $C_k$: $p(x|C_k) = \\mathcal{N}(x|\\mu_k, S_W)$. Where $\\mathcal{N}(x|\\mu_k, S_W)$ s the multivariate Gaussian distribution defined over a P-dimensional vector $x$ of continuous variables, which is given by\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu_k, S_W) = \\frac{1}{(2\\pi)^{P/2}|S_W|^{1/2}}\\exp\\{-\\frac{1}{2} (x - \\mu)^T S_W^{-1}(x - \\mu)\\}\n",
    "$$\n",
    "\n",
    "2. Estimate the **prior probabilities** of class $k$, $p(C_k) = N_k/N$.\n",
    "\n",
    "3. Compute **posterior probabilities** (ie. the probability of a each class given a sample) combining conditional with priors using Bayes' rule:\n",
    "$$\n",
    "p(C_k|x) = \\frac{p(C_k) p(x|C_k)}{p(x)}\n",
    "$$\n",
    "Where $p(x)$ is the marginal distribution obtained by suming of classes:\n",
    "As usual, the denominator\n",
    "in Bayes’ theorem can be found in terms of the quantities appearing in the\n",
    "numerator, because\n",
    "$$\n",
    "p(x) = \\sum_k p(x|C_k)p(C_k)\n",
    "$$\n",
    "4. Classify $x$ using the Maximum-a-Posteriori probability: $C_k= \\arg \\max_{C_k} p(C_k|x)$\n",
    "\n",
    "LDA is a generative approach since the class-conditional distributions cal be used to generate samples of each classes.\n",
    "LDA is useful to deal with imbalanced group sizes (eg.: $N_1 >> N_0$) since priors probabilities can be used to explicitly re-balance the classification by setting $p(C_0) = p(C_1) = 1/2$ or whatever seems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=10, error rate=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.5/site-packages/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "# Dataset\n",
    "n_samples, n_features = 100, 2\n",
    "mean0, mean1 = np.array([0, 0]), np.array([0, 2])\n",
    "Cov = np.array([[1, .8],[.8, 1]])\n",
    "np.random.seed(42)\n",
    "X0 = np.random.multivariate_normal(mean0, Cov, n_samples)\n",
    "X1 = np.random.multivariate_normal(mean1, Cov, n_samples)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * X0.shape[0] + [1] * X1.shape[0])\n",
    "\n",
    "# LDA with scikit-learn\n",
    "lda = LDA()\n",
    "proj = lda.fit(X, y).transform(X)\n",
    "y_pred = lda.predict(X)\n",
    "\n",
    "errors =  y_pred != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

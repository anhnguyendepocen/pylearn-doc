{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis (LDA)\n",
    "\n",
    "Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, that find a linear combination of features that discriminate samples from two or more classes. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\n",
    "\n",
    "### Fisher's linear discriminant with equal class covariance\n",
    "\n",
    "This geometric method does not make probabilistic assumption, it only relies on distances. It look for the linear projection (rotation) $\\mathbf{w}$ that maximizes the between / within variance ratio: noted $F(w)$. It should be considered as a pedagogical experimental method. However with few assumptions it will provide the same results than LDA.\n",
    "\n",
    "Suppose two classes ($C_0, C_1$) of observations have means $\\mu_0$, $\\mu_1$ and the same the \n",
    "total within-class scatter ``covariance'' matrix $S_W$ given by:\n",
    "\\begin{align}\n",
    "S_W &= \\sum_{i\\in C_0} (x_i - \\mu_0)(x_i - \\mu_0)^T + \\sum_{j\\in C_1} (x_j - \\mu_1)(x_j -\\mu_1)^T\\\\\n",
    "    &= X_c^T X_c\n",
    "\\end{align}\n",
    "\n",
    "Where $X_c$ is the $(N \\times P)$ matrix of data centered on their respective means:\n",
    "\n",
    "$$\n",
    "X_c = \n",
    "\\begin{bmatrix}\n",
    "X_0 -  \\mu_0 \\\\ X_1 -  \\mu_1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $X_0$ and $X_1$ are the $(N_0 \\times P)$ and $(N_1 \\times P)$ matrices of samples of classes $C_0$ and $C_1$.\n",
    "\n",
    "Let $S_B$ being the scatter ``between-class'' covariance matrix and given by\n",
    "\n",
    "$$\n",
    "S_B = (\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T\n",
    "$$\n",
    "\n",
    "\n",
    "The linear combination of features $w^T x$ have means $w^T \\mu_i$ for i=0,1 and variance $w^T \n",
    "X^T_c X_c w$. Fisher defined the separation between these two distributions to be the ratio of the \n",
    "variance between the classes to the variance within the classes:\n",
    "\n",
    "\\begin{align}\n",
    "F_{\\text{Fisher}}(w) &= \\frac{\\sigma_{\\text{between}}^2}{\\sigma_{\\text{within}}^2}\\\\\n",
    "                     &= \\frac{(w^T \\mu_1 - w^T \\mu_0)^2}{w^T  X^T_c X_c w}\\\\\n",
    "                     &= \\frac{(w^T (\\mu_1 - \\mu_0))^2}{w^T  X^T_c X_c w}\\\\ \n",
    "                     &= \\frac{w^T (\\mu_1 - \\mu_0) (\\mu_1 - \\mu_0)^T w}{w^T X^T_c X_c w}\\\\\n",
    "                     &= \\frac{w^T S_B w}{w^T S_W w}\n",
    "\\end{align}\n",
    "\n",
    "#### Theorem\n",
    "\n",
    "In the two classes case, the maximum separation occurs by a projection on the $(\\mu_1 - \\mu_0)$ using the Mahalanobis \n",
    "metric $S_B^{-1}$:\n",
    "\n",
    "$$\n",
    "    w \\propto S_B^{-1}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "####Â Demonstration\n",
    "\n",
    "Differentiating $F_{Fisher}(w)$ with respect to $w$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{w}F_{Fisher}(w) &= 0\\\\\n",
    "\\nabla_{w}(\\frac{w^T S_B w}{w^T S_W w}) &= 0\\\\\n",
    "(w^T S_W w)(2 S_B w) - (w^T S_B w)(2 S_W w) &= 0\\\\\n",
    "(w^T S_W w)(S_B w) &= (w^T S_B w)(S_W w)\\\\\n",
    "S_B w &= \\frac{w^T S_B w}{w^T S_W w}(S_W w)\\\\\n",
    "S_B w &= \\lambda (S_W w)\\\\\n",
    "S_W^{-1}{S_B} w &= \\lambda  w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since we do not care about the magnitude of $w$, only its direction, we replaced the scalar factor $(w^T S_B w) / (w^T S_W w)$ by $\\lambda$. \n",
    "\n",
    "Note that this almost looks like an eigen-value equation, if the matrix $S_W^{-1} S_B$ would have \n",
    "been symmetric (in fact, it is called a generalized eigen-problem).\n",
    "\n",
    "However, in the two classes case (where $S_B = (\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T$), it is easy to \n",
    "show that $w = S_W^{-1}(\\mu_1 - \\mu_0)$ is the eigen-vector of this equation:\n",
    "\n",
    "\\begin{align*}\n",
    "S_W^{-1}(\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T w &= \\lambda  w\\\\\n",
    "S_W^{-1}(\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T S_W^{-1}(\\mu_1 - \\mu_0) &= \\lambda  S_W^{-1}(\\mu_1 \n",
    "- \\mu_0)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where here $\\lambda = (\\mu_1 - \\mu_0 )^T S_W^{-1}(\\mu_1 - \\mu_0)$. Which leads to the result:\n",
    "$$\n",
    "    w \\propto S_B^{-1}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "The threshold can be chosen as the hyperplane between projections of the two means:\n",
    "$$\n",
    "T = w \\cdot \\frac{1}{2}(\\mu_1 - \\mu_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification with Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification with scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classification\n",
    "\n",
    "A linear classifier achieves its classification decision $\\hat{y_i}$ based on the value of a linear combination of the input features of a given sample $x_i$.\n",
    "$$\n",
    "\\hat{y_i} = f(w \\cdot x_i)\n",
    "$$\n",
    "Where $w \\cdot x_i ::= w^Tx_i$ is the dot product bewteen $w$ and $x_i$.\n",
    "\n",
    "Linear classifers learners looks for a weights vector $w$ that minimizes a given loss function: $L(y_i, \\hat{y_i})$ and some penalties on the weights vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's linear discriminant with equal class covariance\n",
    "\n",
    "This geometric method does not make probabilistic assumption, it only relies on distances. It look for the **linear projection** (rotation) $\\mathbf{w}$ that maximizes the between / within variance ratio: noted $F(w)$. It should be considered as a pedagogical experimental method. However with few assumptions it will provide the same results than Linear discriminant analysis (LDA) explained below.\n",
    "\n",
    "Suppose two classes ($C_0, C_1$) of observations have means $\\mu_0$, $\\mu_1$ and the same the \n",
    "total within-class scatter ``covariance'' matrix $S_W$ given by:\n",
    "\\begin{align}\n",
    "S_W &= \\sum_{i\\in C_0} (x_i - \\mu_0)(x_i - \\mu_0)^T + \\sum_{j\\in C_1} (x_j - \\mu_1)(x_j -\\mu_1)^T\\\\\n",
    "    &= X_c^T X_c\n",
    "\\end{align}\n",
    "\n",
    "Where $X_c$ is the $(N \\times P)$ matrix of data centered on their respective means:\n",
    "\n",
    "$$\n",
    "X_c = \n",
    "\\begin{bmatrix}\n",
    "X_0 -  \\mu_0 \\\\ X_1 -  \\mu_1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $X_0$ and $X_1$ are the $(N_0 \\times P)$ and $(N_1 \\times P)$ matrices of samples of classes $C_0$ and $C_1$.\n",
    "\n",
    "Let $S_B$ being the scatter ``between-class'' covariance matrix and given by\n",
    "\n",
    "$$\n",
    "S_B = (\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T\n",
    "$$\n",
    "\n",
    "\n",
    "The linear combination of features $w^T x$ have means $w^T \\mu_i$ for i=0,1 and variance $w^T \n",
    "X^T_c X_c w$. Fisher defined the separation between these two distributions to be the ratio of the \n",
    "variance between the classes to the variance within the classes:\n",
    "\n",
    "\\begin{align}\n",
    "F_{\\text{Fisher}}(w) &= \\frac{\\sigma_{\\text{between}}^2}{\\sigma_{\\text{within}}^2}\\\\\n",
    "                     &= \\frac{(w^T \\mu_1 - w^T \\mu_0)^2}{w^T  X^T_c X_c w}\\\\\n",
    "                     &= \\frac{(w^T (\\mu_1 - \\mu_0))^2}{w^T  X^T_c X_c w}\\\\ \n",
    "                     &= \\frac{w^T (\\mu_1 - \\mu_0) (\\mu_1 - \\mu_0)^T w}{w^T X^T_c X_c w}\\\\\n",
    "                     &= \\frac{w^T S_B w}{w^T S_W w}\n",
    "\\end{align}\n",
    "\n",
    "### The Fisher most discriminant projection\n",
    "\n",
    "In the two classes case, the maximum separation occurs by a projection on the $(\\mu_1 - \\mu_0)$ using the Mahalanobis \n",
    "metric $S_B^{-1}$:\n",
    "\n",
    "$$\n",
    "    w \\propto S_W^{-1}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "Differentiating $F_{Fisher}(w)$ with respect to $w$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{w}F_{Fisher}(w) &= 0\\\\\n",
    "\\nabla_{w}(\\frac{w^T S_B w}{w^T S_W w}) &= 0\\\\\n",
    "(w^T S_W w)(2 S_B w) - (w^T S_B w)(2 S_W w) &= 0\\\\\n",
    "(w^T S_W w)(S_B w) &= (w^T S_B w)(S_W w)\\\\\n",
    "S_B w &= \\frac{w^T S_B w}{w^T S_W w}(S_W w)\\\\\n",
    "S_B w &= \\lambda (S_W w)\\\\\n",
    "S_W^{-1}{S_B} w &= \\lambda  w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since we do not care about the magnitude of $w$, only its direction, we replaced the scalar factor $(w^T S_B w) / (w^T S_W w)$ by $\\lambda$. \n",
    "\n",
    "In the multiple $K$-classes case, the solutions $w$ are determined by the eigenvectors of $S_W^{-1}{S_B}$ that correspond to the ($K-1$) largest eigenvalues.\n",
    "\n",
    "However, in the two classes case (where $S_B = (\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T$) it is easy to \n",
    "show that $w = S_W^{-1}(\\mu_1 - \\mu_0)$ is the unique eigenvector of $S_W^{-1}{S_B}$:\n",
    "\n",
    "\\begin{align*}\n",
    "S_W^{-1}(\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T w &= \\lambda  w\\\\\n",
    "S_W^{-1}(\\mu_1 - \\mu_0 )(\\mu_1 - \\mu_0 )^T S_W^{-1}(\\mu_1 - \\mu_0) &= \\lambda  S_W^{-1}(\\mu_1 \n",
    "- \\mu_0)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where here $\\lambda = (\\mu_1 - \\mu_0 )^T S_W^{-1}(\\mu_1 - \\mu_0)$. Which leads to the result:\n",
    "$$\n",
    "    w \\propto S_W^{-1}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "### The separating hyperplane\n",
    "\n",
    "The separating hyperplane is a plan $P-1$-dimensional hyper surface orthogonal to the projection vector: $w$. To define the origin of the plane along $w$. This origin is the classification threshold to decide whether a point should be classified $C_0$ or $C_1$. The threshold can be chosen as the hyperplane between projections of the two means:\n",
    "$$\n",
    "T = w \\cdot \\frac{1}{2}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "![The Fisher most discriminant projection](images/fisher_linear_disc.png)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write an class ``FisherLinearDiscriminant`` that implements the Fisher's linear discriminant analysis. This class must be compliant with the scikit-learn API by providing two methods: \n",
    "- ``fit(X, y)`` which fits the model and returns the object itself;\n",
    "- ``predict(X)`` which returns a vector of the predicted values.\n",
    "Apply the objet on the dataset presented for the LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis (LDA)\n",
    "\n",
    "Linear discriminant analysis (LDA) is a probabilistic generalization of Fisher's linear discriminant. It uses Bayes' rule to fix the threshold based on prior probabilities of classes. \n",
    "\n",
    "1. First compute the The class-**conditional distributions** of $x$ given class $C_k$: $p(x|C_k) = \\mathcal{N}(x|\\mu_k, S_W)$. Where $\\mathcal{N}(x|\\mu_k, S_W)$ is the multivariate Gaussian distribution defined over a P-dimensional vector $x$ of continuous variables, which is given by\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu_k, S_W) = \\frac{1}{(2\\pi)^{P/2}|S_W|^{1/2}}\\exp\\{-\\frac{1}{2} (x - \\mu_k)^T S_W^{-1}(x - \\mu_k)\\}\n",
    "$$\n",
    "\n",
    "2. Estimate the **prior probabilities** of class $k$, $p(C_k) = N_k/N$.\n",
    "\n",
    "3. Compute **posterior probabilities** (ie. the probability of a each class given a sample) combining conditional with priors using Bayes' rule:\n",
    "$$\n",
    "p(C_k|x) = \\frac{p(C_k) p(x|C_k)}{p(x)}\n",
    "$$\n",
    "Where $p(x)$ is the marginal distribution obtained by suming of classes:\n",
    "As usual, the denominator\n",
    "in Bayes’ theorem can be found in terms of the quantities appearing in the\n",
    "numerator, because\n",
    "$$\n",
    "p(x) = \\sum_k p(x|C_k)p(C_k)\n",
    "$$\n",
    "\n",
    "4. Classify $x$ using the Maximum-a-Posteriori probability: $C_k= \\arg \\max_{C_k} p(C_k|x)$\n",
    "\n",
    "LDA is a **generative model** since the class-conditional distributions cal be used to generate samples of each classes.\n",
    "\n",
    "LDA is useful to deal with imbalanced group sizes (eg.: $N_1 \\gg N_0$) since priors probabilities can be used to explicitly re-balance the classification by setting $p(C_0) = p(C_1) = 1/2$ or whatever seems relevant.\n",
    "\n",
    "LDA can be generalised to the mulicasses case with $K>2$.\n",
    "\n",
    "With  $N_1 = N_0$, LDA lead to the same solution than Fisher's linear discriminant.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "How many parameters are required to estimate to perform a LDA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=10, error rate=0.05\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Dataset\n",
    "n_samples, n_features = 100, 2\n",
    "mean0, mean1 = np.array([0, 0]), np.array([0, 2])\n",
    "Cov = np.array([[1, .8],[.8, 1]])\n",
    "np.random.seed(42)\n",
    "X0 = np.random.multivariate_normal(mean0, Cov, n_samples)\n",
    "X1 = np.random.multivariate_normal(mean1, Cov, n_samples)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * X0.shape[0] + [1] * X1.shape[0])\n",
    "\n",
    "# LDA with scikit-learn\n",
    "lda = LDA()\n",
    "proj = lda.fit(X, y).transform(X)\n",
    "y_pred_lda = lda.predict(X)\n",
    "\n",
    "errors =  y_pred_lda != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_lda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is called a generalized linear models. ie.: it is a linear model with a link function that maps the output of linear multiple regression to a the posterior probability of each class $p(C_k|x) \\in [0, 1]$ using the logistic sigmoid function:\n",
    "\n",
    "$$\n",
    "p(C_k|w, x_i) = \\frac{1}{1 + \\exp(-w \\cdot x_i)}\n",
    "$$\n",
    "\n",
    "![logistic sigmoid function](images/logistic.png)\n",
    "\n",
    "Logistic regression seeks to minimizes the likelihood as **Loss function**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{min}~L(w) = \\Pi_i^N p(C_k|w, x_i)\\\\\n",
    "$$\n",
    "Using the log-likelihood:\n",
    "$$\n",
    "\\text{min}~\\log L(w) = \\sum_i^N \\log p(C_k|w, x_i)\\\\\n",
    "$$\n",
    "\n",
    "In the two-class case the algorithms simplify considerably by coding the two-classes ($C_0$ and $C_1$) via a 0/1 response $y_i$. Indeed, since $p(C_0|w, x_i) = 1 - p(C_1|w, x_i)$, the log-likelihood can be re-witten:\n",
    "\n",
    "\\begin{align}\n",
    "\\log L(w) &=  \\sum_i^N\\{y_i \\log p(C_1|w, x_i) + (1 - y_i) \\log(1 - p(C_1|w, x_i))\\}\\\\\n",
    "\\log L(w) &= \\sum_i^N\\{y_i~w \\cdot x_i - \\log(1 + \\exp^{w \\cdot x_i})\\}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Logistic regression is a **discriminative model** since it focuses only on the posterior probability of each class $p(C_k|x)$. It only requires to estimate the $P$ weight of the $w$ vector. Thus it should be favoured over LDA with many input features. In small dimension and balanced situations it would provide similar predictions than LDA.\n",
    "\n",
    "However imbalanced group sizes cannot be explicitly controlled. It can be managed using a reweighting of the input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=10, error rate=0.05\n",
      "[[-5.15162649  5.57299286]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(C=1e8)\n",
    "# This class implements regularized logistic regression. C is the Inverse of regularization strength.\n",
    "# Large value => no regularization.\n",
    "\n",
    "logreg.fit(X, y)\n",
    "y_pred_logreg = logreg.predict(X)\n",
    "\n",
    "errors =  y_pred_logreg != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_logreg)))\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Explore the ``Logistic Regression`` parameters and proposes a solution in cases of highly imbalanced training dataset $N_1 \\gg N_0$  when we know that in reality both classes have the same probability $p(C_1) = p(C_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "VC dimension (for Vapnik–Chervonenkis dimension) is a measure of the **capacity** (complexity, expressive power, richness, or flexibility) of a statistical classification algorithm, defined as the cardinality of the largest set of points that the algorithm can shatter.\n",
    "\n",
    "Theorem: Linear classifier in $R^P$ have VC dimension of $P+1$. Hence in dimension two ($P=2$) any random partition of 3 points can be learned.\n",
    "\n",
    "![In 2D we can shatter any three non-collinear points](images/vc_dimension_linear_2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Fisher's linear classification (L2-regularization)\n",
    "\n",
    "When the matrix $S_W$ is not full rank or $P \\gg N$, the The Fisher most discriminant projection estimate of the is not unique. This can be solved using a biased version of $S_W$:\n",
    "$$\n",
    "S_W^{Ridge} = S_W + \\lambda I\n",
    "$$\n",
    "\n",
    "where $I$ is the $P \\times P$ identity matrix. This leads to the regularized (ridge) estimator of the Fisher's linear discriminant analysis: \n",
    "$$\n",
    "    w^{Ridge} \\propto (S_W + \\lambda I)^{-1}(\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "![The Ridge Fisher most discriminant projection](images/ridge_fisher_linear_disc.png)\n",
    "\n",
    "Increasing $\\lambda$ will:\n",
    "\n",
    "- Shrinks the coeficients toward zero.\n",
    "- The covariance will converge toward the diagnonal matrix, reducing the contribution of the pairwise covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ridge logistic regression (L2-regularization)\n",
    "\n",
    "The **objective function** to be minimized is now the combination of the logistic loss $\\log L(w)$ with a penalty of the L2 norm of the weights vector. In the the two-class case, using the 0/1 coding we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\min~F_{\\text{Logistic Ridge}}(w) &= \\text{Loss}(w)  + \\lambda~\\text{penalty}(w)\\\\\n",
    "                 &= \\log L(w) + \\lambda~\\text{Euclidian norm}(w)^2\\\\\n",
    "                 &= \\sum_i^N\\{y_i w^T x_i - \\log(1 + \\exp^{w^Tx_i})\\} + \\lambda~w \\cdot w\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# Build a classification task using 3 informative features\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100,\n",
    "                           n_features=20,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=0,\n",
    "                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=12, error rate=0.12\n",
      "[[-1.48893004  1.17384218 -0.25707425  0.33929453 -0.32497616  0.01940525\n",
      "   0.25175788  0.12395444 -0.17160509  0.21591212  0.12306606  0.44086876\n",
      "   0.63915726  0.19568658 -0.23394212 -0.06018231  0.05676198  0.23810215\n",
      "   0.59371799  0.37783349]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression(C=1)\n",
    "# This class implements regularized logistic regression. C is the Inverse of regularization strength.\n",
    "# Large value => no regularization.\n",
    "\n",
    "lr.fit(X, y)\n",
    "y_pred_lr = lr.predict(X)\n",
    "\n",
    "errors =  y_pred_lr != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y)))\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso logistic regression (L1-regularization)\n",
    "\n",
    "The **objective function** to be minimized is now the combination of the logistic loss $\\log L(w)$ with a penalty of the L1 norm of the weights vector. In the the two-class case, using the 0/1 coding we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\min~F_{\\text{Logistic Ridge}}(w) &= \\log L(w) + \\lambda~\\text{L1 norm}(w)\\\\\n",
    "                 &= \\sum_i^N\\{y_i~w \\cdot x_i - \\log(1 + \\exp^{w \\cdot x_i})\\} + \\lambda~||w||_1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=12, error rate=0.12\n",
      "[[-1.48145452  1.15758711 -0.20868822  0.20982669 -0.24281     0.\n",
      "   0.14961927  0.01466687 -0.06751532  0.12213636  0.05444326  0.3626353\n",
      "   0.65845587  0.04258666 -0.18660103  0.          0.          0.18521654\n",
      "   0.49627797  0.29068851]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lrl1 = linear_model.LogisticRegression(penalty='l1')\n",
    "# This class implements regularized logistic regression. C is the Inverse of regularization strength.\n",
    "# Large value => no regularization.\n",
    "\n",
    "lrl1.fit(X, y)\n",
    "y_pred_lrl1 = lrl1.predict(X)\n",
    "\n",
    "errors =  y_pred_lrl1 != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_lrl1)))\n",
    "print(lrl1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ridge linear Support Vector Machine (L2-regularization)\n",
    "\n",
    "Support Vector Machine seek for separating hyperplane with maximum margin to enforce robustness against noise. Like logistic regression it is a **discriminative method** that only focuses of predictions.\n",
    "\n",
    "Here we present the non separable case of Maximum Margin Classifiers with $\\pm 1$ coding (ie.: $y_i \\ \\{-1, +1\\}$). In the next figure the legend aply to samples of \"dot\" class. \n",
    "\n",
    "![Linear lar margin classifiers](images/svm.png)\n",
    "\n",
    "Linear SVM for classification (also called SVM-C or SVC) minimizes:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\text{min}   & F_{\\text{Linear SVM}}(w) &= \\text{penalty}(w) +  C~\\text{Loss}(w)\\\\\n",
    "             & & = \\lambda~||w||_2 + C~\\sum_i^N\\xi_i\\\\\n",
    "\\text{with}  & \\forall i & y_i (w \\cdot x_i) \\geq 1 - \\xi_i\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here we introduced the slack variables: $\\xi_i$, with $\\xi_i = 0$ for points that are on or inside the correct margin boundary and $\\xi_i = |y_i - (w \\ cdot  \\cdot x_i)|$ for other points. Thus:\n",
    "\n",
    "1. If $y_i (w \\cdot x_i) \\geq 1$ then the point lies outside the margin but on the correct side of the decision boundary. In this case $\\xi_i=0$. The constraint is thus not active for this point. It does not contribute to the prediction.\n",
    "\n",
    "2. If $1 > y_i (w \\cdot x_i) \\geq 0$ then the point lies inside the margin and on the correct side of the decision boundary. In this case $0<\\xi_i \\leq 1$. The constraint is active for this point. It does contribute to the prediction as a support vector.\n",
    "\n",
    "3. If $0 <  y_i (w \\cdot x_i)$) then the point is on the wrong side of the decision boundary (missclassification). In this case $0<\\xi_i > 1$. The constraint is active for this point. It does contribute to the prediction as a support vector.\n",
    "\n",
    "This loss is called the hinge loss, defined as:\n",
    "\n",
    "$$\n",
    "\\max(0, 1 - y_i~ (w \\cdot x_i))\n",
    "$$\n",
    "\n",
    "So linear SVM is closed to Ridge logistic regression, using the hing loss instead of the logistic loss. Both will provide very similar predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=11, error rate=0.11\n",
      "[[-0.64432158  0.5126233  -0.1150937   0.15219365 -0.14237172 -0.01050273\n",
      "   0.08018828  0.03850495 -0.08353737  0.10839596  0.08814386  0.18741909\n",
      "   0.27257434  0.08486205 -0.08697517 -0.0512884   0.01844099  0.06140848\n",
      "   0.26757953  0.19147258]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svmlin = svm.LinearSVC()\n",
    "# Remark: by default LinearSVC uses squared_hinge as loss\n",
    "svmlin.fit(X, y)\n",
    "y_pred_svmlin = svmlin.predict(X)\n",
    "\n",
    "errors =  y_pred_svmlin != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_svmlin)))\n",
    "print(svmlin.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso linear Support Vector Machine (L1-regularization)\n",
    "\n",
    "Linear SVM for classification (also called SVM-C or SVC) with l1-regularization\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\text{min}   & F_{\\text{Lasso linear SVM}}(w) &= \\lambda~||w||_1 + C~\\sum_i^N\\xi_i\\\\\n",
    "\\text{with}  & \\forall i & y_i (w \\cdot x_i) \\geq 1 - \\xi_i\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb errors=11, error rate=0.11\n",
      "[[-0.61336852  0.4960013  -0.10386448  0.13799837 -0.12950555  0.\n",
      "   0.06952572  0.02368448 -0.06446097  0.09483718  0.07566757  0.1720377\n",
      "   0.26211143  0.07081883 -0.08283651 -0.0330565   0.          0.06155367\n",
      "   0.23894525  0.16745093]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svmlinl1 = svm.LinearSVC(penalty='l1', dual=False)\n",
    "# Remark: by default LinearSVC uses squared_hinge as loss\n",
    "\n",
    "svmlinl1.fit(X, y)\n",
    "y_pred_svmlinl1 = svmlinl1.predict(X)\n",
    "\n",
    "errors =  y_pred_svmlinl1 != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_svmlinl1)))\n",
    "print(svmlinl1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Compare Logistic regression (LR) and their SVM counterparts, ie.: L2 LR vs L2 SVM and L1 LR vs L1 SVM\n",
    "\n",
    "- Compute the correlation between pairs of weights vectors.\n",
    "\n",
    "- Compare the predictions of two classifiers using their decision function: \n",
    "    * Provide the generic form of the decision function for a linear classifier, assuming that their is no intercept.\n",
    "    * Compute the correlation decision function.\n",
    "    * Plot the pairwise decision function of the classifiers.\n",
    "\n",
    "- Conclude on the differences between Linear SVM and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics of classification performance evaluation\n",
    "\n",
    "\n",
    "### Metrics for binary classification\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "\n",
    "Imagine a study evaluating a new test that screens people for a disease. Each person taking the test either has or does not have the disease. The test outcome can be positive (classifying the person as having the disease) or negative (classifying the person as not having the disease). The test results for each subject may or may not match the subject's actual status. In that setting:\n",
    "\n",
    "- True positive (TP): Sick people correctly identified as sick\n",
    "\n",
    "- False positive (FP): Healthy people incorrectly identified as sick\n",
    "\n",
    "- True negative (TN): Healthy people correctly identified as healthy\n",
    "\n",
    "- False negative (FN): Sick people incorrectly identified as healthy\n",
    "\n",
    "- **Accuracy** (ACC):\n",
    "\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "\n",
    "- **Sensitivity** (SEN) or **recall** of the positive class or true positive rate (TPR) or hit rate:\n",
    "\n",
    "    SEN = TP / P = TP / (TP+FN)\n",
    "\n",
    "\n",
    "- **Specificity** (SPC) or **recall** of the negative class or true negative rate:\n",
    "\n",
    "    SPC = TN / N = TN / (TN+FP) \n",
    "\n",
    "\n",
    "- **Precision** or positive predictive value (PPV):\n",
    "\n",
    "    PPV = TP / (TP + FP)\n",
    "\n",
    "\n",
    "- **Balanced accuracy** (bACC):is a useful performance measure is the balanced accuracy which avoids inflated performance estimates on imbalanced datasets (Brodersen, et al. (2010). \"The balanced accuracy and its posterior distribution\"). It is defined as the arithmetic mean of sensitivity and specificity, or the average accuracy obtained on either class:\n",
    "\n",
    "    bACC = 1/2 * (SEN + SPC) \n",
    "\n",
    "- F1 Score (or F-score) which is a weighted average of precision and recall are usefull to deal with imballaced datasets\n",
    "    \n",
    "The four outcomes can be formulated in a 2×2 contingency table or confusion matrix https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "\n",
    "For more precision see: http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "\n",
    "metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "# The overall precision an recall\n",
    "metrics.precision_score(y_true, y_pred)\n",
    "metrics.recall_score(y_true, y_pred)\n",
    "\n",
    "# Recalls on individual classes: SEN & SPC\n",
    "recalls = metrics.recall_score(y_true, y_pred, average=None)\n",
    "recalls[0] # is the recall of class 0: specificity\n",
    "recalls[1] # is the recall of class 1: sensitivity\n",
    "\n",
    "# Balanced accuracy\n",
    "b_acc = recalls.mean()\n",
    "\n",
    "# The overall precision an recall on each individual class\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under Curve (AUC) of Receiver operating characteristic (ROC)\n",
    "\n",
    "Some classifier may have found a good discriminative projection $w$. However if the threshold to decide the final predicted class is poorly adjusted, the performances will highlight an high specificity and a low sensitivity or the contrary.\n",
    "\n",
    "In this case it is recommended to use the AUC of a ROC analysis which basically provide a measure of overlap of the two classes when points are projected on the discriminative axis. For more detail on ROC and AUC see:https://en.wikipedia.org/wiki/Receiver_operating_characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 0 0]\n",
      "Recalls: [ 1.  0.]\n",
      "AUC: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed203246/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "score_pred = np.array([.1 ,.2, .3, .4, .5, .6, .7, .8])\n",
    "y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "thres = .9\n",
    "y_pred = (score_pred > thres).astype(int)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n",
    "metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "# The overall precision an recall on each individual class\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "print(\"Recalls:\", r)\n",
    "# 100% of specificity, 0% of sensitivity\n",
    "\n",
    "# However AUC=1 indicating a perfect separation of the two classes\n",
    "auc = metrics.roc_auc_score(y_true, score_pred)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced classes\n",
    "\n",
    "Learning with discriminative (logistic regression, SVM) methods is generally based on minimizing the misclassification of training samples, which may be unsuitable for imbalanced datasets where the recognition might be biased in favor of\n",
    "the most numerous class. This problem can be addressed with a generative approach, which typically requires\n",
    "more parameters to be determined leading to reduced performances in high dimension.\n",
    "\n",
    "Dealing with imbalanced class may be addressed by three main ways (see Japkowicz and Stephen (2002) for a review), resampling, reweighting and one class learning.\n",
    "\n",
    "In **sampling strategies**, either the minority class is oversampled or majority class is undersampled or some combination of the two is deployed. Undersampling (Zhang and Mani, 2003) the majority class would lead to a poor usage of the left-out samples. Sometime one cannot afford such strategy since we are also facing a small sample size problem even for the majority class.\n",
    "Informed oversampling, which goes beyond a trivial duplication of minority class samples, requires the estimation of class conditional distributions in order to generate synthetic samples. Here generative models are required. An alternative, proposed in (Chawla et al., 2002) generate samples along the line segments joining any/all of the k minority class nearest neighbors. Such procedure blindly generalizes the minority area without regard to the majority class, which may be particularly problematic with high-dimensional and potentially skewed class distribution. \n",
    "\n",
    "**Reweighting**, also called cost-sensitive learning, works at an algorithmic level by adjusting the costs of the various classes to counter the class imbalance. Such reweighting can be implemented within SVM (Chang and Lin, 2001) or logistic regression (Friedman et al., 2010) classifiers. Most classiers of Scikit learn offer such reweighting possibilities. \n",
    "\n",
    "The ``class_weight`` parameter can be positionned into the ``\"balanced\"`` mode which uses the values of $y$ to automatically adjust weights inversely proportional to class frequencies in the input data as $N / (2 N_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples of class 0 = 250; #samples of class 1 = 250;\n",
      "# No Reweighting balanced dataset\n",
      "SPC: 0.940; SEN: 0.928\n",
      "# => The predictions are balanced in sensitivity and specificity\n",
      "\n",
      "#samples of class 0 = 12; #samples of class 1 = 250;\n",
      "# No Reweighting on imbalanced dataset\n",
      "SPC: 0.750; SEN: 0.992\n",
      "# => Sensitivity >> specificity\n",
      "\n",
      "# Reweighting on imbalanced dataset\n",
      "SPC: 1.000; SEN: 0.972\n",
      "# => The predictions are balanced in sensitivity and specificity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset\n",
    "X, y = datasets.make_classification(n_samples=500,\n",
    "                           n_features=5,\n",
    "                           n_informative=2,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=1,\n",
    "                           shuffle=False)\n",
    "\n",
    "print(*[\"#samples of class %i = %i;\" % (lev, np.sum(y == lev)) for lev in np.unique(y)])\n",
    "\n",
    "print('# No Reweighting balanced dataset')\n",
    "lr_inter = linear_model.LogisticRegression(C=1)\n",
    "lr_inter.fit(X, y)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y, lr_inter.predict(X))\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => The predictions are balanced in sensitivity and specificity\\n')\n",
    "\n",
    "# Create imbalanced dataset, by subsampling sample of calss 0: keep only 10% of\n",
    "# classe 0's samples and all classe 1's samples.\n",
    "n0 = int(np.rint(np.sum(y == 0) / 20))\n",
    "subsample_idx = np.concatenate((np.where(y == 0)[0][:n0], np.where(y == 1)[0]))\n",
    "Ximb = X[subsample_idx, :]\n",
    "yimb = y[subsample_idx]\n",
    "print(*[\"#samples of class %i = %i;\" % (lev, np.sum(yimb == lev)) for lev in \n",
    "        np.unique(yimb)])\n",
    "\n",
    "print('# No Reweighting on imbalanced dataset')\n",
    "lr_inter = linear_model.LogisticRegression(C=1)\n",
    "lr_inter.fit(Ximb, yimb)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(yimb, lr_inter.predict(Ximb))\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => Sensitivity >> specificity\\n')\n",
    "\n",
    "print('# Reweighting on imbalanced dataset')\n",
    "lr_inter_reweight = linear_model.LogisticRegression(C=1, class_weight=\"balanced\")\n",
    "lr_inter_reweight.fit(Ximb, yimb)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(yimb, \n",
    "                                                     lr_inter_reweight.predict(Ximb))\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => The predictions are balanced in sensitivity and specificity\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
